# BioALBERT
BioALBERT- A simple and effective pre-trained biomedical language representation model


This repository provides the pre-trained BioALBERT models, a biomedical language representation model trained on large domain specific (biomedical) corpora for designed for biomedical text mining tasks. Please refer to our paper [title of the paper](links) for more details.


## Download

We provide four versions of pre-trained weights. Pre-training was based on the original ALBERT code, and training details are described in our paper (To be Published). Currently available versions of pre-trained weights are as follows:

1) *[BioALBERT-Base v1.0 (PubMed)]() - based on ALBERT-base Model*

2) *[BioALBERT-Base v1.0 (PubMed + PMC)]() - based on ALBERT-base Model*

3) *[BioALBERT-Large v1.1 (PubMed)]() - based on ALBERT-Large Model*

4) *[BioALBERT-Large v1.1 (PubMed + PMC)]() - based on ALBERT-Large Model*

Alternately, you can download pre-trained weights from [here]()

Make sure to specify the version of the pre-trained weights used in your work. 


## Installation

The following sections introduce the installation and fine-tuning process of BioALBERT based on PyTorch (python version <= 3.7).

To fine-tune BioALBERT, you need to download BioALBERT pre-training weights. After downloading the pre-trained weights, install BioALBERT using requirements.txt as follows:

```
git clone https://github.com/usmaann/BioALBERT.git
cd BioALBERT; pip install -r requirements.txt

```

## Quick Links

| Link | Detail |
| --- | --- |
| git status | List all new or modified files |
| git diff | Show file differences that haven't been staged |

## Datasets

## Fine-tuning BioBERT

###several datasets



